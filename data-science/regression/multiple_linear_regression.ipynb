{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Multiple linear regression\n",
    "\n",
    "5 methods of building models\n",
    "- All in\n",
    " - Throw all variables\n",
    " - When to use it?\n",
    "    - Prior knowledge\n",
    "    - You have to use these variables\n",
    "    - Preparing for Backward elimination\n",
    "- Backward Elimination\n",
    " 1. Select a significance model (ex: SL = 0.05)\n",
    " 2. Fit the full model with all possible predictors (all in)\n",
    " 3. Consider the predictor with the highest P-value. \n",
    " If PL > SL, go to step 4. Otherwise, the model is ready\n",
    " 4. Remove the predictor\n",
    " 5. Fit model without this variable. Go to 3.\n",
    "- Forward Selection\n",
    " 1. Select a significance model (ex: SL = 0.05)\n",
    " 2. Fit all simple regression models. Select the one with the lowest P-value\n",
    " 3. Keep this variable and fit all possible models with one extra predictor added to the one you already have.\n",
    " 4. Consider the predictor with the lowest P-value. If P < SL, go to step 3, otherwise keep the previous model\n",
    "- Bidirectional Elimination\n",
    "- Score Comparison"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "labelenconder_X = LabelEncoder()\n",
    "X[:,3] = labelenconder_X.fit_transform(X[:,3])\n",
    "onehotenconder = ColumnTransformer([(\"encoder\", OneHotEncoder(), [3])], remainder=\"passthrough\")\n",
    "X = onehotenconder.fit_transform(X) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avoiding the Dummy Variable Trap\n",
    "Removing the first column"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "X = X[:,1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Splitting the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fitting Multiple Linear Regression to the Training Set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 51
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predicting the Test set results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building the optimal model using Backward Elimination"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.951\nModel:                            OLS   Adj. R-squared:                  0.945\nMethod:                 Least Squares   F-statistic:                     169.9\nDate:                Sun, 29 Mar 2020   Prob (F-statistic):           1.34e-27\nTime:                        01:04:33   Log-Likelihood:                -525.38\nNo. Observations:                  50   AIC:                             1063.\nDf Residuals:                      44   BIC:                             1074.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\nx1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\nx2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\nx3             0.8060      0.046     17.369      0.000       0.712       0.900\nx4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\nx5             0.0270      0.017      1.574      0.123      -0.008       0.062\n==============================================================================\nOmnibus:                       14.782   Durbin-Watson:                   1.283\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\nSkew:                          -0.948   Prob(JB):                     2.41e-05\nKurtosis:                       5.572   Cond. No.                     1.45e+06\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.45e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   169.9</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 29 Mar 2020</td> <th>  Prob (F-statistic):</th> <td>1.34e-27</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>01:04:33</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1063.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   1074.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 5.013e+04</td> <td> 6884.820</td> <td>    7.281</td> <td> 0.000</td> <td> 3.62e+04</td> <td>  6.4e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>  198.7888</td> <td> 3371.007</td> <td>    0.059</td> <td> 0.953</td> <td>-6595.030</td> <td> 6992.607</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>  -41.8870</td> <td> 3256.039</td> <td>   -0.013</td> <td> 0.990</td> <td>-6604.003</td> <td> 6520.229</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.369</td> <td> 0.000</td> <td>    0.712</td> <td>    0.900</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.517</td> <td> 0.608</td> <td>   -0.132</td> <td>    0.078</td>\n</tr>\n<tr>\n  <th>x5</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.574</td> <td> 0.123</td> <td>   -0.008</td> <td>    0.062</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>14.782</td> <th>  Durbin-Watson:     </th> <td>   1.283</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.266</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.41e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.572</td> <th>  Cond. No.          </th> <td>1.45e+06</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.45e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 53
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "X = np.append(arr = np.ones((50,1)).astype(int), values=X,axis=1)\n",
    "X_opt = X[:, [0,1,2,3,4,5]]\n",
    "X_opt = np.array(X_opt, dtype=float)\n",
    "regressor_ols = sm.OLS(endog=y,exog=X_opt).fit()\n",
    "regressor_ols.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove predictor x2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.951\nModel:                            OLS   Adj. R-squared:                  0.946\nMethod:                 Least Squares   F-statistic:                     217.2\nDate:                Sun, 29 Mar 2020   Prob (F-statistic):           8.49e-29\nTime:                        01:04:46   Log-Likelihood:                -525.38\nNo. Observations:                  50   AIC:                             1061.\nDf Residuals:                      45   BIC:                             1070.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       5.011e+04   6647.870      7.537      0.000    3.67e+04    6.35e+04\nx1           220.1585   2900.536      0.076      0.940   -5621.821    6062.138\nx2             0.8060      0.046     17.606      0.000       0.714       0.898\nx3            -0.0270      0.052     -0.523      0.604      -0.131       0.077\nx4             0.0270      0.017      1.592      0.118      -0.007       0.061\n==============================================================================\nOmnibus:                       14.758   Durbin-Watson:                   1.282\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.172\nSkew:                          -0.948   Prob(JB):                     2.53e-05\nKurtosis:                       5.563   Cond. No.                     1.40e+06\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.4e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.946</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   217.2</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 29 Mar 2020</td> <th>  Prob (F-statistic):</th> <td>8.49e-29</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>01:04:46</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1061.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th> <td>   1070.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 5.011e+04</td> <td> 6647.870</td> <td>    7.537</td> <td> 0.000</td> <td> 3.67e+04</td> <td> 6.35e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>  220.1585</td> <td> 2900.536</td> <td>    0.076</td> <td> 0.940</td> <td>-5621.821</td> <td> 6062.138</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.606</td> <td> 0.000</td> <td>    0.714</td> <td>    0.898</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.523</td> <td> 0.604</td> <td>   -0.131</td> <td>    0.077</td>\n</tr>\n<tr>\n  <th>x4</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.592</td> <td> 0.118</td> <td>   -0.007</td> <td>    0.061</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>14.758</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.172</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.53e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.563</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 54
    }
   ],
   "source": [
    "X_opt = X[:, [0,1,3,4,5]]\n",
    "X_opt = np.array(X_opt, dtype=float)\n",
    "regressor_ols = sm.OLS(endog=y,exog=X_opt).fit()\n",
    "regressor_ols.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove predictor x1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.951\nModel:                            OLS   Adj. R-squared:                  0.948\nMethod:                 Least Squares   F-statistic:                     296.0\nDate:                Sun, 29 Mar 2020   Prob (F-statistic):           4.53e-30\nTime:                        01:05:08   Log-Likelihood:                -525.39\nNo. Observations:                  50   AIC:                             1059.\nDf Residuals:                      46   BIC:                             1066.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\nx1             0.8057      0.045     17.846      0.000       0.715       0.897\nx2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\nx3             0.0272      0.016      1.655      0.105      -0.006       0.060\n==============================================================================\nOmnibus:                       14.838   Durbin-Watson:                   1.282\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\nSkew:                          -0.949   Prob(JB):                     2.21e-05\nKurtosis:                       5.586   Cond. No.                     1.40e+06\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.4e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   296.0</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 29 Mar 2020</td> <th>  Prob (F-statistic):</th> <td>4.53e-30</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>01:05:08</td>     <th>  Log-Likelihood:    </th> <td> -525.39</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1066.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 5.012e+04</td> <td> 6572.353</td> <td>    7.626</td> <td> 0.000</td> <td> 3.69e+04</td> <td> 6.34e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    0.8057</td> <td>    0.045</td> <td>   17.846</td> <td> 0.000</td> <td>    0.715</td> <td>    0.897</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>   -0.0268</td> <td>    0.051</td> <td>   -0.526</td> <td> 0.602</td> <td>   -0.130</td> <td>    0.076</td>\n</tr>\n<tr>\n  <th>x3</th>    <td>    0.0272</td> <td>    0.016</td> <td>    1.655</td> <td> 0.105</td> <td>   -0.006</td> <td>    0.060</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>14.838</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.442</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.949</td> <th>  Prob(JB):          </th> <td>2.21e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.586</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 55
    }
   ],
   "source": [
    "X_opt = X[:, [0,3,4,5]]\n",
    "X_opt = np.array(X_opt, dtype=float)\n",
    "regressor_ols = sm.OLS(endog=y,exog=X_opt).fit()\n",
    "regressor_ols.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove predictor x4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.950\nModel:                            OLS   Adj. R-squared:                  0.948\nMethod:                 Least Squares   F-statistic:                     450.8\nDate:                Sun, 29 Mar 2020   Prob (F-statistic):           2.16e-31\nTime:                        01:05:20   Log-Likelihood:                -525.54\nNo. Observations:                  50   AIC:                             1057.\nDf Residuals:                      47   BIC:                             1063.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\nx1             0.7966      0.041     19.266      0.000       0.713       0.880\nx2             0.0299      0.016      1.927      0.060      -0.001       0.061\n==============================================================================\nOmnibus:                       14.677   Durbin-Watson:                   1.257\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\nSkew:                          -0.939   Prob(JB):                     2.54e-05\nKurtosis:                       5.575   Cond. No.                     5.32e+05\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.32e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.950</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.948</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   450.8</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 29 Mar 2020</td> <th>  Prob (F-statistic):</th> <td>2.16e-31</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>01:05:20</td>     <th>  Log-Likelihood:    </th> <td> -525.54</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1057.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    47</td>      <th>  BIC:               </th> <td>   1063.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 4.698e+04</td> <td> 2689.933</td> <td>   17.464</td> <td> 0.000</td> <td> 4.16e+04</td> <td> 5.24e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    0.7966</td> <td>    0.041</td> <td>   19.266</td> <td> 0.000</td> <td>    0.713</td> <td>    0.880</td>\n</tr>\n<tr>\n  <th>x2</th>    <td>    0.0299</td> <td>    0.016</td> <td>    1.927</td> <td> 0.060</td> <td>   -0.001</td> <td>    0.061</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>14.677</td> <th>  Durbin-Watson:     </th> <td>   1.257</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.161</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.939</td> <th>  Prob(JB):          </th> <td>2.54e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.575</td> <th>  Cond. No.          </th> <td>5.32e+05</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.32e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 56
    }
   ],
   "source": [
    "X_opt = X[:, [0,3,5]]\n",
    "X_opt = np.array(X_opt, dtype=float)\n",
    "regressor_ols = sm.OLS(endog=y,exog=X_opt).fit()\n",
    "regressor_ols.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove predictor x5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.947\nModel:                            OLS   Adj. R-squared:                  0.945\nMethod:                 Least Squares   F-statistic:                     849.8\nDate:                Sun, 29 Mar 2020   Prob (F-statistic):           3.50e-32\nTime:                        01:07:02   Log-Likelihood:                -527.44\nNo. Observations:                  50   AIC:                             1059.\nDf Residuals:                      48   BIC:                             1063.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\nx1             0.8543      0.029     29.151      0.000       0.795       0.913\n==============================================================================\nOmnibus:                       13.727   Durbin-Watson:                   1.116\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\nSkew:                          -0.911   Prob(JB):                     9.44e-05\nKurtosis:                       5.361   Cond. No.                     1.65e+05\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.65e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"",
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.947</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   849.8</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Sun, 29 Mar 2020</td> <th>  Prob (F-statistic):</th> <td>3.50e-32</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>01:07:02</td>     <th>  Log-Likelihood:    </th> <td> -527.44</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   1063.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 4.903e+04</td> <td> 2537.897</td> <td>   19.320</td> <td> 0.000</td> <td> 4.39e+04</td> <td> 5.41e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    0.8543</td> <td>    0.029</td> <td>   29.151</td> <td> 0.000</td> <td>    0.795</td> <td>    0.913</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>13.727</td> <th>  Durbin-Watson:     </th> <td>   1.116</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  18.536</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.911</td> <th>  Prob(JB):          </th> <td>9.44e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.361</td> <th>  Cond. No.          </th> <td>1.65e+05</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.65e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 57
    }
   ],
   "source": [
    "X_opt = X[:, [0,3]]\n",
    "X_opt = np.array(X_opt, dtype=float)\n",
    "regressor_ols = sm.OLS(endog=y,exog=X_opt).fit()\n",
    "regressor_ols.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}